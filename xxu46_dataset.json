[{"urllink": "http://arxiv.org/abs/1412.2227", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1412.2227", "title": "\nHellmann-Feynman connection for the relative Fisher information", "abstract": "The reciprocity relations for the relative Fisher information (RFI, hereafter) and a generalized RFI-Euler theorem, are self-consistently derived from the Hellmann-Feynman theorem. These new reciprocity relations generalize the RFI-Euler theorem and constitute the basis for building up a mathematical Legendre transform structure (LTS, hereafter), akin to that of thermodynamics, that underlies the RFI scenario. This demonstrates the possibility of translating the entire mathematical structure of thermodynamics into a RFI-based theoretical framework. Virial theorems play a prominent role in this endeavor, as a Schr \"odinger-like equation can be associated to the RFI. Lagrange multipliers are determined invoking the RFI-LTS link and the quantum mechanical virial theorem. An appropriate ansatz allows for the inference of probability density functions (pdf's, hereafter) and energy-eigenvalues of the above mentioned Schr \"odinger-like equation. The energy-eigenvalues obtained here via inference are benchmarked against established theoretical and numerical results. A principled theoretical basis to reconstruct the RFI-framework from the FIM framework is established. Numerical examples for exemplary cases are provided.", "subjects": "Statistical Mechanics (cond-mat.stat-mech)", "authors": "R. C. Venkatesan, A. Plastino,"}, 
{"urllink": "http://arxiv.org/abs/1501.07084", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1501.07084", "title": "\nk2U: A General Framework from k-Point Effective Schedulability Analysis  to Utilization-Based Tests", "abstract": "To deal with a large variety of workloads in different application domains in real-time embedded systems, a number of expressive task models have been developed. For each individual task model, researchers tend to develop different types of techniques for schedulability tests with different computation complexity and performance. In this paper, we present a general schedulability analysis framework, namely the k2U framework, that can be potentially applied to analyze a large set of real- time task models under any fixed-priority scheduling algorithm, on both uniprocessors and multiprocessors. The key to k2U is a k-point effective schedulability test, which can be viewed as a blackbox interface to apply the k2U framework. For any task model, if a corresponding k-point effective schedulability test can be constructed, then a sufficient utilization-based test can be automatically derived. We show the generality of k2U by applying it to different task models, which results in new and better tests compared to the state-of-the-art.", "subjects": "Operating Systems (cs.OS)", "authors": "Jian-Jia Chen, Wen-Hung Huang, Cong Liu,"}, 
{"urllink": "http://arxiv.org/abs/1503.02143", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1503.02143", "title": "\nModel selection of polynomial kernel regression", "abstract": "Polynomial kernel regression is one of the standard and state-of-the-art learning strategies. However, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. The first aim of this paper is to develop a strategy to select these parameters. On one hand, based on the worst-case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. In other words, the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On the other hand,taking account of the implementation of the algorithm, the regularization term is required. Summarily, the effect of the regularization term in polynomial kernel regression is only to circumvent the \" ill-condition\" of the kernel matrix. Based on this, the second purpose of this paper is to propose a new model selection strategy, and then design an efficient learning algorithm. Both theoretical and experimental analysis show that the new strategy outperforms the previous one. Theoretically, we prove that the new learning strategy is almost optimal if the regression function is smooth. Experimentally, it is shown that the new strategy can significantly reduce the computational burden without loss of generalization capability.", "subjects": "Learning (cs.LG)", "authors": "Shaobo Lin, Xingping Sun, Zongben Xu, Jinshan Zeng,"}, 
{"urllink": "http://arxiv.org/abs/1411.4314", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1411.4314", "title": "\nHierarchical and Matrix Structures in a Large Organizational Email  Network: Visualization and Modeling Approaches", "abstract": "This paper presents findings from a study of the email network of a large scientific research organization, focusing on methods for visualizing and modeling organizational hierarchies within large, complex network datasets. In the first part of the paper, we find that visualization and interpretation of complex organizational network data is facilitated by integration of network data with information on formal organizational divisions and levels. By aggregating and visualizing email traffic between organizational units at various levels, we derive several insights into how large subdivisions of the organization interact with each other and with outside organizations. Our analysis shows that line and program management interactions in this organization systematically deviate from the idealized pattern of interaction prescribed by \"matrix management.\" In the second part of the paper, we propose a power law model for predicting degree distribution of organizational email traffic based on hierarchical relationships between managers and employees. This model considers the influence of global email announcements sent from managers to all employees under their supervision, and the role support staff play in generating email traffic, acting as agents for managers. We also analyze patterns in email traffic volume over the course of a work week.", "subjects": "Social and Information Networks (cs.SI)", "authors": "Benjamin H. Sims, Nikolai Sinitsyn, Stephan J. Eidenbenz,"}, 
{"urllink": "http://arxiv.org/abs/1406.0132", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0132", "title": "\nSeeing the Big Picture: Deep Embedding with Contextual Evidences", "abstract": "In the Bag-of-Words (BoW) model based image retrieval task, the precision of visual matching plays a critical role in improving retrieval performance. Conventionally, local cues of a keypoint are employed. However, such strategy does not consider the contextual evidences of a keypoint, a problem which would lead to the prevalence of false matches. To address this problem, this paper defines \"true match\" as a pair of keypoints which are similar on three levels, i.e., local, regional, and global. Then, a principled probabilistic framework is established, which is capable of implicitly integrating discriminative cues from all these feature levels. Specifically, the Convolutional Neural Network (CNN) is employed to extract features from regional and global patches, leading to the so-called \"Deep Embedding\" framework. CNN has been shown to produce excellent performance on a dozen computer vision tasks such as image classification and detection, but few works have been done on BoW based image retrieval. In this paper, firstly we show that proper pre-processing techniques are necessary for effective usage of CNN feature. Then, in the attempt to fit it into our model, a novel indexing structure called \"Deep Indexing\" is introduced, which dramatically reduces memory usage. Extensive experiments on three benchmark datasets demonstrate that, the proposed Deep Embedding method greatly promotes the retrieval accuracy when CNN feature is integrated. We show that our method is efficient in terms of both memory and time cost, and compares favorably with the state-of-the-art methods.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "authors": "Liang Zheng, Shengjin Wang, Fei He, Qi Tian,"}, 
{"urllink": "http://arxiv.org/abs/1405.2564", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2564", "title": "\nTowards an Efficient Prolog System by Code Introspection", "abstract": "To appear in Theory and Practice of Logic Programming (TPLP). Several Prolog interpreters are based on the Warren Abstract Machine (WAM), an elegant model to compile Prolog programs. In order to improve the performance several strategies have been proposed, such as: optimize the selection of clauses, specialize the unification, global analysis, native code generation and tabling. This paper proposes a different strategy to implement an efficient Prolog System, the creation of specialized emulators on the fly. The proposed strategy was implemented and evaluated at YAP Prolog System, and the experimental evaluation showed interesting results.", "subjects": "Programming Languages (cs.PL)", "authors": "George Souza Oliveira, Anderson Faustino da Silva,"}, 
{"urllink": "http://arxiv.org/abs/1406.7459", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7459", "title": "\nSpeedup of Micromagnetic Simulations with C++ AMP On Graphics Processing  Units", "abstract": "A finite-difference Micromagnetic solver is presented utilizing the C++ Accelerated Massive Parallelism (C++ AMP). The high speed performance of a single Graphics Processing Unit (GPU) is demonstrated compared to a typical CPU-based solver. The speed-up of GPU to CPU is shown to be greater than 100 for problems with larger sizes. This solver is based on C++ AMP and can run on GPUs from various hardware vendors, such as NVIDIA, AMD and Intel, regardless of whether it is dedicated or integrated graphics processor.", "subjects": "Computational Engineering, Finance, and Science (cs.CE)", "authors": "Ru Zhu,"}, 
{"urllink": "http://arxiv.org/abs/1407.5407", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1407.5407", "title": "\nThe Highly Accurate N-DEterminant (HANDE) quantum Monte Carlo project:  Open-source stochastic diagonalisation for quantum chemistry", "abstract": "The HANDE quantum Monte Carlo project offers accessible stochastic diagonalization algorithms for general use for scientists in the field of quantum chemistry. It is an ambitious and general high-performance code developed by a geographically-dispersed team with a variety of backgrounds in computational science. As we prepare for a public, open-source release, we take this as an opportunity to step back and look at what we have done and what we hope to do in the future. We pay particular attention to development processes and the approach taken to train students joining the project.", "subjects": "Software Engineering (cs.SE)", "authors": "J. S. Spencer, N. S. Blunt, W. A. Vigor, F. D. Malone, W. M. C. Foulkes, James J. Shepherd, A. J. W. Thom,"}, 
{"urllink": "http://arxiv.org/abs/1406.0124", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0124", "title": "\nSoftware-Defined Networking: State of the Art and Research Challenges", "abstract": "Plug-and-play information technology (IT) infrastructure has been expanding very rapidly in recent years. With the advent of cloud computing, many ecosystem and business paradigms are encountering potential changes and may be able to eliminate their IT infrastructure maintenance processes. Real-time performance and high availability requirements have induced telecom networks to adopt the new concepts of the cloud model: software-defined networking (SDN) and network function virtualization (NFV). NFV introduces and deploys new network functions in an open and standardized IT environment, while SDN aims to transform the way networks function. SDN and NFV are complementary technologies; they do not depend on each other. However, both concepts can be merged and have the potential to mitigate the challenges of legacy networks. In this paper, our aim is to describe the benefits of using SDN in a multitude of environments such as in data centers, data center networks, and Network as Service offerings. We also present the various challenges facing SDN, from scalability to reliability and security concerns, and discuss existing solutions to these challenges.", "subjects": "Networking and Internet Architecture (cs.NI)", "authors": "Manar Jammal, Taranpreet Singh, Abdallah Shami, Rasool Asal, Yiming Li,"}, 
{"urllink": "http://arxiv.org/abs/1405.2555", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2555", "title": "\nHow to Securely Compute the Modulo-Two Sum of Binary Sources", "abstract": "In secure multiparty computation, mutually distrusting users in a network want to collaborate to compute functions of data which is distributed among the users. The users should not learn any additional information about the data of others than what they may infer from their own data and the functions they are computing. Previous works have mostly considered the worst case context (i.e., without assuming any distribution for the data); Lee and Abbe (2014) is a notable exception. Here, we study the average case (i.e., we work with a distribution on the data) where correctness and privacy is only desired asymptotically. For concreteness and simplicity, we consider a secure version of the function computation problem of K \"orner and Marton (1979) where two users observe a doubly symmetric binary source with parameter p and the third user wants to compute the XOR. We show that the amount of communication and randomness resources required depends on the level of correctness desired. When zero-error and perfect privacy are required, the results of Data et al. (2014) show that it can be achieved if and only if a total rate of 1 bit is communicated between every pair of users and private randomness at the rate of 1 is used up. In contrast, we show here that, if we only want the probability of error to vanish asymptotically in block length, it can be achieved by a lower rate (binary entropy of p) for all the links and for private randomness; this also guarantees perfect privacy. We also show that no smaller rates are possible even if privacy is only required asymptotically.", "subjects": "Information Theory (cs.IT)", "authors": "Deepesh Data, Bikash Kumar Dey, Manoj Mishra, Vinod M. Prabhakaran,"}, 
{"urllink": "http://arxiv.org/abs/1406.7447", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7447", "title": "\nUnimodal Bandits without Smoothness", "abstract": "We consider stochastic bandit problems with a continuous set of arms and where the expected reward is a continuous and unimodal function of the arm. No further assumption is made regarding the smoothness and the structure of the expected reward function. For these problems, we propose the Stochastic Pentachotomy (SP) algorithm, and derive finite-time upper bounds on its regret and optimization error. In particular, we show that, for any expected reward function that behaves as locally around its maximizer for some , the SP algorithm is order-optimal. Namely its regret and optimization error scale as and , respectively, when the time horizon grows large. These scalings are achieved without the knowledge of and . Our algorithm is based on asymptotically optimal sequential statistical tests used to successively trim an interval that contains the best arm with high probability. To our knowledge, the SP algorithm constitutes the first sequential arm selection rule that achieves a regret and optimization error scaling as and , respectively, up to a logarithmic factor for non-smooth expected reward functions, as well as for smooth functions with unknown smoothness.", "subjects": "Learning (cs.LG)", "authors": "Richard Combes, Alexandre Proutiere,"}, 
{"urllink": "http://arxiv.org/abs/1407.5404", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1407.5404", "title": "\nModel based design of super schedulers managing catastrophic scenario in  hard real time systems", "abstract": "The conventional design of real-time approaches depends heavily on the normal performance of systems and it often becomes incapacitated in dealing with catastrophic scenarios effectively. There are several investigations carried out to effectively tackle large scale catastrophe of a plant and how real-time systems must reorganize itself to respond optimally to changing scenarios to reduce catastrophe and aid human intervention. The study presented here is in this direction and the model accommodates catastrophe generated tasks while it tries to minimize the total number of deadline miss, by dynamically scheduling the unusual pattern of tasks. The problem is NP hard. We prove the methods for an optimal scheduling algorithm. We also derive a model to maintain the stability of the processes. Moreover, we study the problem of minimizing the number of processors required for scheduling with a set of periodic and sporadic hard real time tasks with primary/backup mechanism to achieve fault tolerance. EDF scheduling algorithms are used on each processor to manage scenario changes. Finally we present a simulation of super scheduler with small, medium and large real time tasks pattern for catastrophe management.", "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC)", "authors": "A. Christy Persya, T.R. Gopalakrishnan Nair,"}, 
{"urllink": "http://arxiv.org/abs/1404.5701", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1404.5701", "title": "\nAchieving Shannon Capacity in a Wiretap Channel via Previous Messages", "abstract": "In this paper we consider a wiretap channel with a secret key buffer. We use the coding scheme of [1] to enhance the secrecy rate to the capacity of the main channel, while storing each securely transmitted message in the secret key buffer. We use the oldest secret bits from the buffer to be used as a secret key to transmit a message in a slot and then remove those bits. With this scheme we are able to prove stronger results than those in [1]. i.e., not only the message which is being transmitted currently, but all the messages transmitted in last slots are secure with respect to all the information that the eavesdropper possesses, where can be chosen arbitrarily large.", "subjects": "Information Theory (cs.IT)", "authors": "Shahid Mehraj Shah, Vinod Sharma,"}, 
{"urllink": "http://arxiv.org/abs/1406.0117", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0117", "title": "\nEACOF: A Framework for Providing Energy Transparency to enable  Energy-Aware Software Development", "abstract": "Making energy consumption data accessible to software developers is an essential step towards energy efficient software engineering. The presence of various different, bespoke and incompatible, methods of instrumentation to obtain energy readings is currently limiting the widespread use of energy data in software development. This paper presents EACOF, a modular Energy-Aware Computing Framework that provides a layer of abstraction between sources of energy data and the applications that exploit them. EACOF replaces platform specific instrumentation through two APIs - one accepts input to the framework while the other provides access to application software. This allows developers to profile their code for energy consumption in an easy and portable manner using simple API calls. We outline the design of our framework and provide details of the API functionality. In a use case, where we investigate the impact of data bit width on the energy consumption of various sorting algorithms, we demonstrate that the data obtained using EACOF provides interesting, sometimes counter-intuitive, insights. All the code is available online under an open source license. this http URL", "subjects": "Software Engineering (cs.SE)", "authors": "Hayden Field, Glen Anderson, Kerstin Eder,"}, 
{"urllink": "http://arxiv.org/abs/1405.2553", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2553", "title": "\nGraph Spectral Properties of Deterministic Finite Automata", "abstract": "We prove that a minimal automaton has a minimal adjacency matrix rank and a minimal adjacency matrix nullity using equitable partition (from graph spectra theory) and Nerode partition (from automata theory). This result naturally introduces the notion of matrix rank into a regular language L, the minimal adjacency matrix rank of a deterministic automaton that recognises L. We then define and focus on rank-one languages: the class of languages for which the rank of minimal automaton is one. We also define the expanded canonical automaton of a rank-one language.", "subjects": "Formal Languages and Automata Theory (cs.FL)", "authors": "Ryoma Sin'ya,"}, 
{"urllink": "http://arxiv.org/abs/1406.7445", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7445", "title": "\nContrastive Feature Induction for Efficient Structure Learning of  Conditional Random Fields", "abstract": "Structure learning of Conditional Random Fields (CRFs) can be cast into an L1-regularized optimization problem. To avoid optimizing over a fully linked model, gain-based or gradient-based feature selection methods start from an empty model and incrementally add top ranked features to it. However, for high-dimensional problems like statistical relational learning, training time of these incremental methods can be dominated by the cost of evaluating the gain or gradient of a large collection of candidate features. In this study we propose a fast feature evaluation algorithm called Contrastive Feature Induction (CFI), which only evaluates a subset of features that involve both variables with high signals (deviation from mean) and variables with high errors (residue). We prove that the gradient of candidate features can be represented solely as a function of signals and errors, and that CFI is an efficient approximation of gradient-based evaluation methods. Experiments on synthetic and real data sets show competitive learning speed and accuracy of CFI on pairwise CRFs, compared to state-of-the-art structure learning methods such as full optimization over all features, and Grafting.", "subjects": "Learning (cs.LG)", "authors": "Ni Lao, Jun Zhu,"}, 
{"urllink": "http://arxiv.org/abs/1407.5399", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1407.5399", "title": "\nLow-Effort Specification Debugging and Analysis", "abstract": "Reactive synthesis deals with the automated construction of implementations of reactive systems from their specifications. To make the approach feasible in practice, systems engineers need effective and efficient means of debugging these specifications. In this paper, we provide techniques for report-based specification debugging, wherein salient properties of a specification are analyzed, and the result presented to the user in the form of a report. This provides a low-effort way to debug specifications, complementing high-effort techniques including the simulation of synthesized implementations. We demonstrate the usefulness of our report-based specification debugging toolkit by providing examples in the context of generalized reactivity(1) synthesis.", "subjects": "Software Engineering (cs.SE)", "authors": "R\u00fcdiger Ehlers, Vasumathi Raman,"}, 
{"urllink": "http://arxiv.org/abs/1404.5692", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1404.5692", "title": "\nForward - Backward Greedy Algorithms for Atomic Norm Regularization", "abstract": "In many signal processing applications, one aims to reconstruct a signal that has a simple representation with respect to a certain basis or frame. Fundamental elements of the basis known as \"atoms\" allow us to define \"atomic norms\" that can be used to construct convex regularizers for the reconstruction problem. Efficient algorithms are available to solve the reconstruction problems in certain special cases, but an approach that works well for general atomic norms remains to be found. This paper describes an optimization algorithm called CoGEnT , which produces solutions with succinct atomic representations for reconstruction problems, generally formulated with atomic norm constraints. CoGEnT combines a greedy selection scheme based on the conditional gradient approach with a backward (or \"truncation\") step that exploits the quadratic nature of the objective to reduce the basis size. We establish convergence properties and validate the algorithm via extensive numerical experiments on a suite of signal processing applications. Our algorithm and analysis are also novel in that they allow for inexact forward steps. In practice, CoGEnT significantly outperforms the basic conditional gradient method, and indeed many methods that are tailored to specific applications, when the truncation steps are defined appropriately. We also introduce several novel applications that are enabled by the atomic norm framework, including tensor completion, moment problems in signal processing, and graph deconvolution.", "subjects": "Data Structures and Algorithms (cs.DS)", "authors": "Nikhil Rao, Parikshit Shah, Stephen Wright,"}, 
{"urllink": "http://arxiv.org/abs/1406.0090", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0090", "title": "\nError Control Codes: A Novel Solution for Secret Key Generation and Key  Refreshment Problem", "abstract": "Cryptography is the science of encrypting the information so that it is rendered unreadable for an intruder. Cryptographic techniques are of utmost importance in today's world as the information to be sent might be of invaluable importance to both the sender and the receiver. Various cryptographic techniques ensure that even if an intruder intercepts the sent information, he is not able to decipher it thus render ending it useless for the intruder. Cryptography can be grouped into two types, that is Symmetric key cryptography and Asymmetric key cryptography. Symmetric key cryptography uses the same key for encryption as well as decryption thus making it faster compared to Asymmetric Key cryptography which uses different keys for encryption and decryption. Generation of dynamic keys for Symmetric key cryptography is an interesting field and in this we have tapped this field so as to generate dynamic keys for symmetric key cryptography. In this work, we have devised an algorithm for generating dynamic keys for sending messages over a communication channel and also solving key refreshment problem.", "subjects": "Cryptography and Security (cs.CR)", "authors": "Arjun Puri, Sudesh Kumar,"}, 
{"urllink": "http://arxiv.org/abs/1405.2539", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2539", "title": "\nA Review of Image Mosaicing Techniques", "abstract": "Image Mosaicing is a method of constructing multiple images of the same scene into a larger image. The output of the image mosaic will be the union of two input images. Image-mosaicing algorithms are used to get mosaiced image. Image Mosaicing processed is basically divided in to 5 phases. Which includes; Feature point extraction, Image registration, Homography computation, Warping and Blending if Image. Various corner detection algorithm is being used for Feature extraction. This corner produces an efficient and informative output mosaiced image. Image mosaicing is widely used in creating 3D images, medical imaging, computer vision, data from satellites, and military automatic target recognition.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "authors": "Dushyant Vaghela, Prof. Kapildev Naina,"}, 
{"urllink": "http://arxiv.org/abs/1406.0175", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0175", "title": "\nEvolutionary Search in the Space of Rules for Creation of New Two-Player  Board Games", "abstract": "Games have always been a popular test bed for artificial intelligence techniques. Game developers are always in constant search for techniques that can automatically create computer games minimizing the developer's task. In this work we present an evolutionary strategy based solution towards the automatic generation of two player board games. To guide the evolutionary process towards games, which are entertaining, we propose a set of metrics. These metrics are based upon different theories of entertainment in computer games. This work also compares the entertainment value of the evolved games with the existing popular board based games. Further to verify the entertainment value of the evolved games with the entertainment value of the human user a human user survey is conducted. In addition to the user survey we check the learnability of the evolved games using an artificial neural network based controller. The proposed metrics and the evolutionary process can be employed for generating new and entertaining board games, provided an initial search space is given to the evolutionary algorithm.", "subjects": "Neural and Evolutionary Computing (cs.NE)", "authors": "Zahid Halim,"}, 
{"urllink": "http://arxiv.org/abs/1406.0231", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0231", "title": "\nAmbiguous Proximity Distribution", "abstract": "Proximity Distribution Kernel is an effective method for bag-of-featues based image representation. In this paper, we investigate the soft assignment of visual words to image features for proximity distribution. Visual word contribution function is proposed to model ambiguous proximity distributions. Three ambiguous proximity distributions is developed by three ambiguous contribution functions. The experiments are conducted on both classification and retrieval of medical image data sets. The results show that the performance of the proposed methods, Proximity Distribution Kernel (PDK), is better or comparable to the state-of-the-art bag-of-features based image representation methods.", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "authors": "Quanquan Wang, Yongping Li,"}, 
{"urllink": "http://arxiv.org/abs/1406.0296", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0296", "title": "\nUsing Mobile Agents for Information Retrival in B2B Systems", "abstract": "This paper presents an architecture of an information retrieval system that use the advantages offered by mobile agents to collect information from different sources and bring the result to the calling user. Mobile agent technology will be used for determine the traceability of a product and also for searching information about a specific entity.", "subjects": "Information Retrieval (cs.IR)", "authors": "Felicia Florentina Giza, Cristina Elena Turcu, Ovidiu Andrei Schipor,"}, 
{"urllink": "http://arxiv.org/abs/1406.0373", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0373", "title": "\nLinear Programming Relaxations for Goldreich's Generators over  Non-Binary Alphabets", "abstract": "Goldreich suggested candidates of one-way functions and pseudorandom generators included in . It is known that randomly generated Goldreich's generator using -wise independent predicates with input variables and output variables is not pseudorandom generator with high probability for sufficiently large constant . Most of the previous works assume that the alphabet is binary and use techniques available only for the binary alphabet. In this paper, we deal with non-binary generalization of Goldreich's generator and derives the tight threshold for linear programming relaxation attack using local marginal polytope for randomly generated Goldreich's generators. We assume that input variables are known. In that case, we show that when , there is an exact threshold such that for , the LP relaxation can determine linearly many input variables of Goldreich's generator if , and that the LP relaxation cannot determine input variables of Goldreich's generator if . This paper uses characterization of LP solutions by combinatorial structures called stopping sets on a bipartite graph, which is related to a simple algorithm called peeling algorithm.", "subjects": "Cryptography and Security (cs.CR)", "authors": "Ryuhei Mori, Takeshi Koshiba, Osamu Watanabe, Masaki Yamamoto,"}, 
{"urllink": "http://arxiv.org/abs/1406.0554", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0554", "title": "\nUniversal Convexification via Risk-Aversion", "abstract": "We develop a framework for convexifying a fairly general class of optimization problems. Under additional assumptions, we analyze the suboptimality of the solution to the convexified problem relative to the original nonconvex problem and prove additive approximation guarantees. We then develop algorithms based on stochastic gradient methods to solve the resulting optimization problems and show bounds on convergence rates. %We show a simple application of this framework to supervised learning, where one can perform integration explicitly and can use standard (non-stochastic) optimization algorithms with better convergence guarantees. We then extend this framework to apply to a general class of discrete-time dynamical systems. In this context, our convexification approach falls under the well-studied paradigm of risk-sensitive Markov Decision Processes. We derive the first known model-based and model-free policy gradient optimization algorithms with guaranteed convergence to the optimal solution. Finally, we present numerical results validating our formulation in different applications.", "subjects": "Systems and Control (cs.SY)", "authors": "Krishnamurthy Dvijotham, Maryam Fazel, Emanuel Todorov,"}, 
{"urllink": "http://arxiv.org/abs/1406.0532", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0532", "title": "\nMultimodal vs. Unimodal Physiological Control in Videogames for Enhanced  Realism and Depth", "abstract": "(arXiv abridged abstract) In the last two decades, videogames have evolved in a nearly explosive way from the pixelated graphics to today's near-realistic 3D environments. The interaction devices traditionally used in videogames have not evolved with the same intensity, but recent HCI studies have explored biofeedback interaction - the explicit manipulation of a person's physiological data as input to a system - as an alternative to them. Traditional biofeedback prototypes apply 1 sensor to each game mechanic (unimodality). In this dissertation, we introduce the combination of 2 physiological sensors simultaneously per game mechanic (multimodality) and present a First-Person Shooter game comprised of 8 game mechanics with three interaction flavours (no biofeedback/vanilla, unimodal and multimodal). An empirical study with 32 regular players was employed to explore and study differences between the three interaction types and where they can be best employed. Players compared the three games in terms of Fun, Ease of Use, Originality, Playability and Favourite Condition. For the sake of completeness, other evaluation methods were used as well: IMI Questionnaire, keywords association and open-ended commentaries. The vanilla version was considered easier to use, but both biofeedback versions were considered the most fun. Both versions were praised differently: the unimodal version for its simplicity of use, and the multimodal for its realism, activation safety of game mechanics and depth added to the game. Our conclusion is that multimodal biofeedback can have a relevant impact in terms of added depth, depending on the way it is used inside the game. On a boundary case, it can be used to increase the feeling of empowerment on the player when using certain abilities, or to intentionally make in-game actions more difficult by demanding more physical effort from the player.", "subjects": "Human-Computer Interaction (cs.HC)", "authors": "Gon\u00e7alo Amaral da Silva,"}, 
{"urllink": "http://arxiv.org/abs/1406.0516", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0516", "title": "\nModeling Diffusion of Competing Products and Conventions in Social Media", "abstract": "The emergence and wide-spread use of social networks and microblogging sites has led to a dramatic increase on the availability of users' activity data. Importantly, this data can be exploited to solve some of the problems that have captured the attention of economists and marketers for decades as, e.g., product adoption, product competition and product life cycle. In this paper, we leverage on users' activity data from a popular microblogging site to model and predict the competing dynamics of products and social conventions adoptions. To this aim, we propose a data-driven model, based on continuous-time Hawkes processes, for the adoption and frequency of use of competing products and conventions. We then develop an inference method to efficiently fit the model parameters by solving a convex program. The problem decouples into a collection of smaller subproblems, thus scaling easily to networks with hundred of thousands of nodes. We validate our method over synthetic and real diffusion data gathered from Twitter, and show that the proposed model does not only present a good predictive power but also provides interpretable model parameters, which allow us to gain insights into the fundamental principles that drive product and convention adoptions.", "subjects": "Social and Information Networks (cs.SI)", "authors": "Isabel Valera, Manuel Gomez-Rodriguez, Krishna Gummadi,"}, 
{"urllink": "http://arxiv.org/abs/1406.0495", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0495", "title": "\nAutomatic Recognition of Dyslalia Affecting Pre-Scholars", "abstract": "This article describes the recognition part of a system that will be used for personalized therapy of dyslalia affecting pre scholars. Dyslalia is a speech disorder that affect pronunciation of one ore many sounds. The full system targets interdisciplinary research (computer science, psychology, electronics) - having as main objective the development of methods, models, algorithms, System on Chip architectures with regards to the elaboration and implementation of a complete system addressing the therapy of dyslalia affecting pre scholars, in a personalized and user centered manner. The system addresses the number of 10% of children with age between 4 and 7 that, according to the statistics, present different variations of speech impairments. Although these impairments do not create major difficulties concerning common communication, it has been noticed that problems are likely to appear affecting negatively the child's personality as well as his social environment.", "subjects": "Computers and Society (cs.CY)", "authors": "Stefan-Gheorghe Pentiuc, Ovidiu-Andrei Schipor, Mirela Danubianu, Doina-Maria Schipor,"}, 
{"urllink": "http://arxiv.org/abs/1406.0492", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0492", "title": "\nDijkstra meets Steiner: a fast exact goal-oriented Steiner tree  algorithm", "abstract": "We present a new exact algorithm for the Steiner tree problem in graphs which is based on dynamic programming. Known empirically fast algorithms are primarily based on reductions, heuristics and branching. Our algorithm combines the best known worst-case run time with a fast, often superior, practical performance.", "subjects": "Data Structures and Algorithms (cs.DS)", "authors": "Stefan Hougardy, Jannik Silvanus, Jens Vygen,"}, 
{"urllink": "http://arxiv.org/abs/1405.2872", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2872", "title": "\nAnalysis of Asymptotically Optimal Sampling-based Motion Planning  Algorithms for Lipschitz Continuous Dynamical Systems", "abstract": "Over the last 20 years significant effort has been dedicated to the development of sampling-based motion planning algorithms such as the Rapidly-exploring Random Trees (RRT) and its asymptotically optimal version (e.g. RRT*). However, asymptotic optimality for RRT* only holds for linear and fully actuated systems or for a small number of non-linear systems (e.g. Dubin's car) for which a steering function is available. The purpose of this paper is to show that asymptotically optimal motion planning for dynamical systems with differential constraints can be achieved without the use of a steering function. We develop a novel analysis on sampling-based planning algorithms that sample the control space. This analysis demonstrated that asymptotically optimal path planning for any Lipschitz continuous dynamical system can be achieved by sampling the control space directly. We also determine theoretical bounds on the convergence rates for this class of algorithms. As the number of iterations increases, the trajectory generated by these algorithms, approaches the optimal control trajectory, with probability one. Simulation results are promising.", "subjects": "Robotics (cs.RO)", "authors": "Georgios Papadopoulos, Hanna Kurniawati, Nicholas M. Patrikalakis,"}, 
{"urllink": "http://arxiv.org/abs/1406.0486", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0486", "title": "\nMonte Carlo Tree Search with Heuristic Evaluations using Implicit  Minimax Backups", "abstract": "Monte Carlo Tree Search (MCTS) has improved the performance of game engines in domains such as Go, Hex, and general game playing. MCTS has been shown to outperform classic alpha-beta search in games where good heuristic evaluations are difficult to obtain. In recent years, combining ideas from traditional minimax search in MCTS has been shown to be advantageous in some domains, such as Lines of Action, Amazons, and Breakthrough. In this paper, we propose a new way to use heuristic evaluations to guide the MCTS search by storing the two sources of information, estimated win rates and heuristic evaluations, separately. Rather than using the heuristic evaluations to replace the playouts, our technique backs them up implicitly during the MCTS simulations. These minimax values are then used to guide future simulations. We show that using implicit minimax backups leads to stronger play performance in Kalah, Breakthrough, and Lines of Action.", "subjects": "Artificial Intelligence (cs.AI)", "authors": "Marc Lanctot, Mark H.M. Winands, Tom Pepels, Nathan R. Sturtevant,"}, 
{"urllink": "http://arxiv.org/abs/1405.2861", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2861", "title": "\nSecure Fragmentation for Content-Centric Networks", "abstract": "Content-Centric Networking (CCN) is a communications paradigm that emphasizes content distribution. Named-Data Networking (NDN) is an instantiation of CCN, a candidate Future Internet Architecture. NDN supports human-readable content naming and router-based content caching which lends itself to efficient, secure and scalable content distribution. Because of NDN's fundamental requirement that each content object must be signed by its producer, fragmentation has been considered incompatible with NDN since it precludes authentication of individual content fragments by routers. The alternative of hop-by-hop reassembly is problematic due to the substantial incurred delay. In this paper, we show that secure and efficient content fragmentation is both possible and even advantageous in NDN and similar information-centric architectures that involve signed content. We design a concrete technique that facilitates efficient and secure content fragmentation in NDN, discuss its security guarantees and assess performance. We also describe a prototype implementation and compare performance of cut-through with hop-by-hop fragmentation and reassembly.", "subjects": "Networking and Internet Architecture (cs.NI)", "authors": "Cesar Ghali, Ashok Narayanan, David Oran, Gene Tsudik,"}, 
{"urllink": "http://arxiv.org/abs/1406.0455", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0455", "title": "\nBuyer to Seller Recommendation under Constraints", "abstract": "The majority of recommender systems are designed to recommend items (such as movies and products) to users. We focus on the problem of recommending buyers to sellers which comes with new challenges: (1) constraints on the number of recommendations buyers are part of before they become overwhelmed, (2) constraints on the number of recommendations sellers receive within their budget, and (3) constraints on the set of buyers that sellers want to receive (e.g., no more than two people from the same household). We propose the following critical problems of recommending buyers to sellers: Constrained Recommendation (C-REC) capturing the first two challenges, and Conflict-Aware Constrained Recommendation (CAC-REC) capturing all three challenges at the same time. We show that C-REC can be modeled using linear programming and can be efficiently solved using modern solvers. On the other hand, we show that CAC-REC is NP-hard. We propose two approximate algorithms to solve CAC-REC and show that they achieve close to optimal solutions via comprehensive experiments using real-world datasets.", "subjects": "Social and Information Networks (cs.SI)", "authors": "Cheng Chen, Lan Zheng, Venkatesh Srinivasan, Alex Thomo, Kui Wu, Anthony Sukow,"}, 
{"urllink": "http://arxiv.org/abs/1405.2856", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2856", "title": "\nMapping the UK Webspace: Fifteen Years of British Universities on the  Web", "abstract": "This paper maps the national UK web presence on the basis of an analysis of the .uk domain from 1996 to 2010. It reviews previous attempts to use web archives to understand national web domains and describes the dataset. Next, it presents an analysis of the .uk domain, including the overall number of links in the archive and changes in the link density of different second-level domains over time. We then explore changes over time within a particular second-level domain, the academic subdomain .ac.uk, and compare linking practices with variables, including institutional affiliation, league table ranking, and geographic location. We do not detect institutional affiliation affecting linking practices and find only partial evidence of league table ranking affecting network centrality, but find a clear inverse relationship between the density of links and the geographical distance between universities. This echoes prior findings regarding offline academic activity, which allows us to argue that real-world factors like geography continue to shape academic relationships even in the Internet age. We conclude with directions for future uses of web archive resources in this emerging area of research.", "subjects": "Digital Libraries (cs.DL)", "authors": "Scott A. Hale, Taha Yasseri, Josh Cowls, Eric T. Meyer, Ralph Schroeder, Helen Margetts,"}, 
{"urllink": "http://arxiv.org/abs/1406.7650", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7650", "title": "\nGossip-based Signaling Dissemination Extension for Next Steps In  Signaling", "abstract": "In this paper, we propose a new gossip-based signaling dissemination method for the Next Steps in Signaling protocol family. In more detail, we propose to extend the General Internet Signaling Transport (GIST) protocol, so as to leverage these new dissemination capabilities from all NSIS Signaling Layer Protocol applications using its transport capabilities. The new GIST extension consists of two main procedures: a bootstrap procedure, during which new GIST-enabled nodes discover each other, and a service dissemination procedure, which is used to effectively disseminate signaling messages within an Autonomous System. To this aim, we defined three dissemination models, bubble, balloon, and hose, so as to fulfill requirements of different network and/or service management scenarios. An experimental campaign carried out on the GENI testbed shows the effectiveness of the proposed solution.", "subjects": "Networking and Internet Architecture (cs.NI)", "authors": "M. Femminella, R. Francescangeli, G. Reali, H. Schulzrinne,"}, 
{"urllink": "http://arxiv.org/abs/1406.0440", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0440", "title": "\nSoftware-Defined Networking: A Comprehensive Survey", "abstract": "Software-Defined Networking (SDN) is an emerging paradigm that promises to change this state of affairs, by breaking vertical integration, separating the network's control logic from the underlying routers and switches, promoting (logical) centralization of network control, and introducing the ability to program the network. The separation of concerns introduced between the definition of network policies, their implementation in switching hardware, and the forwarding of traffic, is key to the desired flexibility: by breaking the network control problem into tractable pieces, SDN makes it easier to create and introduce new abstractions in networking, simplifying network management and facilitating network evolution. In this paper we present a comprehensive survey on SDN. We start by introducing the motivation for SDN, explain its main concepts and how it differs from traditional networking, its roots, and the standardization activities regarding this novel paradigm. Next, we present the key building blocks of an SDN infrastructure using a bottom-up, layered approach. We provide an in-depth analysis of the hardware infrastructure, southbound and northbound APIs, network virtualization layers, network operating systems (SDN controllers), network programming languages, and network applications. We also look at cross-layer problems such as debugging and troubleshooting. In an effort to anticipate the future evolution of this new paradigm, we discuss the main ongoing research efforts and challenges of SDN. In particular, we address the design of switches and control platforms -- with a focus on aspects such as resiliency, scalability, performance, security and dependability -- as well as new opportunities for carrier transport networks and cloud providers. Last but not least, we analyze the position of SDN as a key enabler of a software-defined environment.", "subjects": "Networking and Internet Architecture (cs.NI)", "authors": "Diego Kreutz, Fernando M. V. Ramos, Paulo Verissimo, Christian Esteve Rothenberg, Siamak Azodolmolky, Steve Uhlig,"}, 
{"urllink": "http://arxiv.org/abs/1405.2852", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2852", "title": "\nOn the Total Variation Distance of Labelled Markov Chains", "abstract": "Labelled Markov chains (LMCs) are widely used in probabilistic verification, speech recognition, computational biology, and many other fields. Checking two LMCs for equivalence is a classical problem subject to extensive studies, while the total variation distance provides a natural measure for the \"inequivalence\" of two LMCs: it is the maximum difference between probabilities that the LMCs assign to the same event. In this paper we develop a theory of the total variation distance between two LMCs, with emphasis on the algorithmic aspects: (1) we provide a polynomial-time algorithm for determining whether two LMCs have distance 1, i.e., whether they can almost always be distinguished; (2) we provide an algorithm for approximating the distance with arbitrary precision; and (3) we show that the threshold problem, i.e., whether the distance exceeds a given threshold, is NP-hard and hard for the square-root-sum problem. We also make a connection between the total variation distance and Bernoulli convolutions.", "subjects": "Logic in Computer Science (cs.LO)", "authors": "Taolue Chen, Stefan Kiefer,"}, 
{"urllink": "http://arxiv.org/abs/1406.7630", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7630", "title": "\nStochastic stability and stabilization of a class of state-dependent  jump linear systems", "abstract": "This paper deals a continuous-time state-dependent jump linear system, a particular kind of stochastic switching system. In particular, we consider a situation when the transition rate of the random jump process depends on the state variable, and addressed the problem of stochastic stability and stabilization analysis for the proposed system. Numerically solvable ?sufficient conditions for the stochastic stability and stabilization of the proposed system is established in terms of linear matrix inequalities. The obtained results are illustrated in numerical examples.", "subjects": "Systems and Control (cs.SY)", "authors": "Shaikshavali Chitraganti, Samir Aberkane, Christophe Aubrun,"}, 
{"urllink": "http://arxiv.org/abs/1406.0435", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0435", "title": "\nOdysseus/DFS: Integration of DBMS and Distributed File System for  Transaction Processing of Big Data", "abstract": "The relational DBMS (RDBMS) has been widely used since it supports various high-level functionalities such as SQL, schemas, indexes, and transactions that do not exist in the O/S file system. But, a recent advent of big data technology facilitates development of new systems that sacrifice the DBMS functionality in order to efficiently manage large-scale data. Those so-called NoSQL systems use a distributed file system, which support scalability and reliability. They support scalability of the system by storing data into a large number of low-cost commodity hardware and support reliability by storing the data in replica. However, they have a drawback that they do not adequately support high-level DBMS functionality. In this paper, we propose an architecture of a DBMS that uses the DFS as storage. With this novel architecture, the DBMS is capable of supporting scalability and reliability of the DFS as well as high-level functionality of DBMS. Thus, a DBMS can utilize a virtually unlimited storage space provided by the DFS, rendering it to be suitable for big data analytics. As part of the architecture of the DBMS, we propose the notion of the meta DFS file, which allows the DBMS to use the DFS as the storage, and an efficient transaction management method including recovery and concurrency control. We implement this architecture in Odysseus/DFS, an integration of the Odysseus relational DBMS, that has been being developed at KAIST for over 24 years, with the DFS. Our experiments on transaction processing show that, due to the high-level functionality of Odysseus/DFS, it outperforms Hbase, which is a representative open-source NoSQL system. We also show that, compared with an RDBMS with local storage, the performance of Odysseus/DFS is comparable or marginally degraded, showing that the overhead of Odysseus/DFS for supporting scalability by using the DFS as the storage is not significant.", "subjects": "Databases (cs.DB)", "authors": "Jun-Sung Kim, Kyu-Young Whang, Hyuk-Yoon Kwon, Il-Yeol Song,"}, 
{"urllink": "http://arxiv.org/abs/1405.2850", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2850", "title": "\nA Simple and Efficient Lock-Free Hash Trie Design for Concurrent Tabling", "abstract": "A critical component in the implementation of a concurrent tabling system is the design of the table space. One of the most successful proposals for representing tables is based on a two-level trie data structure, where one trie level stores the tabled subgoal calls and the other stores the computed answers. In this work, we present a simple and efficient lock-free design where both levels of the tries can be shared among threads in a concurrent environment. To implement lock-freedom we took advantage of the CAS atomic instruction that nowadays can be widely found on many common architectures. CAS reduces the granularity of the synchronization when threads access concurrent areas, but still suffers from low-level problems such as false sharing or cache memory side-effects. In order to be as effective as possible in the concurrent search and insert operations over the table space data structures, we based our design on a hash trie data structure in such a way that it minimizes potential low-level synchronization problems by dispersing as much as possible the concurrent areas. Experimental results in the Yap Prolog system show that our new lock-free hash trie design can effectively reduce the execution time and scale better than previous designs.", "subjects": "Programming Languages (cs.PL)", "authors": "Miguel Areias, Ricardo Rocha,"}, 
{"urllink": "http://arxiv.org/abs/1406.7629", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7629", "title": "\nOn control of discrete-time state-dependent jump linear systems with  probabilistic constraints: A receding horizon approach", "abstract": "In this article, we consider a receding horizon control of discrete-time state-dependent jump linear systems, particular kind of stochastic switching systems, subject to possibly unbounded random disturbances and probabilistic state constraints. Due to a nature of the dynamical system and the constraints, we consider a one-step receding horizon. Using inverse cumulative distribution function, we convert the probabilistic state constraints to deterministic constraints, and obtain a tractable deterministic receding horizon control problem. We consider the receding control law to have a linear state-feedback and an admissible offset term. We ensure mean square boundedness of the state variable via solving linear matrix inequalities off-line, and solve the receding horizon control problem on-line with control offset terms. We illustrate the overall approach applied on a macroeconomic system.", "subjects": "Systems and Control (cs.SY)", "authors": "Shaikshavali Chitraganti, Samir Aberkane, Christophe Aubrun, Guillermo Valencia-Palomo, Vasile Dragan,"}, 
{"urllink": "http://arxiv.org/abs/1406.0416", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0416", "title": "\nMore Bang For Your Buck: Quorum-Sensing Capabilities Improve the  Efficacy of Suicidal Altruism", "abstract": "Within the context of evolution, an altruistic act that benefits the receiving individual at the expense of the acting individual is a puzzling phenomenon. An extreme form of altruism can be found in colicinogenic E. coli. These suicidal altruists explode, releasing colicins that kill unrelated individuals, which are not colicin resistant. By committing suicide, the altruist makes it more likely that its kin will have less competition. The benefits of this strategy rely on the number of competitors and kin nearby. If the organism explodes at an inopportune time, the suicidal act may not harm any competitors. Communication could enable organisms to act altruistically when environmental conditions suggest that that strategy would be most beneficial. Quorum sensing is a form of communication in which bacteria produce a protein and gauge the amount of that protein around them. Quorum sensing is one means by which bacteria sense the biotic factors around them and determine when to produce products, such as antibiotics, that influence competition. Suicidal altruists could use quorum sensing to determine when exploding is most beneficial, but it is challenging to study the selective forces at work in microbes. To address these challenges, we use digital evolution (a form of experimental evolution that uses self-replicating computer programs as organisms) to investigate the effects of enabling altruistic organisms to communicate via quorum sensing. We found that quorum-sensing altruists killed a greater number of competitors per explosion, winning competitions against non-communicative altruists. These findings indicate that quorum sensing could increase the beneficial effect of altruism and the suite of conditions under which it will evolve.", "subjects": "Neural and Evolutionary Computing (cs.NE)", "authors": "Anya Elaine Johnson, Eli Strauss, Rodney Pickett, Christoph Adami, Ian Dworkin, Heather J. Goldsby,"}, 
{"urllink": "http://arxiv.org/abs/1405.2848", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2848", "title": "\nQuery Rewriting and Optimization for Ontological Databases", "abstract": "Ontological queries are evaluated against a knowledge base consisting of an extensional database and an ontology (i.e., a set of logical assertions and constraints which derive new intensional knowledge from the extensional database), rather than directly on the extensional database. The evaluation and optimization of such queries is an intriguing new problem for database research. In this paper, we discuss two important aspects of this problem: query rewriting and query optimization. Query rewriting consists of the compilation of an ontological query into an equivalent first-order query against the underlying extensional database. We present a novel query rewriting algorithm for rather general types of ontological constraints which is well-suited for practical implementations. In particular, we show how a conjunctive query against a knowledge base, expressed using linear and sticky existential rules, that is, members of the recently introduced Datalog+/- family of ontology languages, can be compiled into a union of conjunctive queries (UCQ) against the underlying database. Ontological query optimization, in this context, attempts to improve this rewriting process so to produce possibly small and cost-effective UCQ rewritings for an input query.", "subjects": "Databases (cs.DB)", "authors": "Georg Gottlob, Giorgio Orsi, Andreas Pieris,"}, 
{"urllink": "http://arxiv.org/abs/1406.7623", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7623", "title": "\nTransmit Designs for the MIMO Broadcast Channel with Statistical CSI", "abstract": "We investigate the multiple-input multiple-output broadcast channel with statistical channel state information available at the transmitter. The so-called linear assignment operation is employed, and necessary conditions are derived for the optimal transmit design under general fading conditions. Based on this, we introduce an iterative algorithm to maximize the linear assignment weighted sum-rate by applying a gradient descent method. To reduce complexity, we derive an upper bound of the linear assignment achievable rate of each receiver, from which a simplified closed-form expression for a near-optimal linear assignment matrix is derived. This reveals an interesting construction analogous to that of dirty-paper coding. In light of this, a low complexity transmission scheme is provided. Numerical examples illustrate the significant performance of the proposed low complexity scheme.", "subjects": "Information Theory (cs.IT)", "authors": "Yongpeng Wu, Shi Jin, Xiqi Gao, Matthew R. McKay, Chengshan Xiao,"}, 
{"urllink": "http://arxiv.org/abs/1406.0403", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0403", "title": "\nOptimizing the flash-RAM energy trade-off in deeply embedded systems", "abstract": "Deeply embedded systems often have the tightest constraints on energy consumption, requiring that they consume tiny amounts of current and run on batteries for years. However, they typically execute code directly from flash, instead of the more energy efficient RAM. We implement a novel compiler optimization that exploits the relative efficiency of RAM by statically moving carefully selected basic blocks from flash to RAM. Our technique uses integer linear programming, with an energy cost model to select a good set of basic blocks to place into RAM, without impacting stack or data storage. We evaluate our optimization on a common ARM microcontroller and succeed in reducing the average power consumption by up to 41% and reducing energy consumption by up to 22%, while increasing execution time. A case study is presented, where an application executes code then sleeps for a period of time. For this example we show that our optimization could allow the application to run on battery for up to 32% longer. We also show that for this scenario the total application energy can be reduced, even if the optimization increases the execution time of the code.", "subjects": "Other Computer Science (cs.OH)", "authors": "James Pallister, Kerstin Eder, Simon Hollis,"}, 
{"urllink": "http://arxiv.org/abs/1406.7620", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7620", "title": "\nJoint Optimization of Spectrum Sensing and Accessing in Multiuser MISO  Cognitive Networks", "abstract": "In this paper, a joint spectrum sensing and accessing optimization framework for a multiuser cognitive network is proposed to significantly improve spectrum efficiency. For such a cognitive network, there are two important and limited resources that should be distributed in a comprehensive manner, namely feedback bits and time duration. First, regarding the feedback bits, there are two components: sensing component (used to convey various users' sensing results) and accessing component (used to feedback channel state information). A large sensing component can support more users to perform cooperative sensing, which results in high sensing precision. However, a large accessing component is preferred as well, as it has a direct impact on the performance in the multiuser cognitive network when multi-antenna technique, such as zero-forcing beamforming (ZFBF), is utilized. Second, the tradeoff of sensing and accessing duration in a transmission interval needs to be determined, so that the sum transmission rate is optimized while satisfying the interference constraint. In addition, the above two resources are interrelated and inversive under some conditions. Specifically, sensing time can be saved by utilizing more sensing feedback bits for a given performance objective. Hence, the resources should be allocation in a jointly manner. Based on the joint optimization framework and the intrinsic relationship between the two resources, we propose two joint resource allocation schemes by maximizing the average sum transmission rate in a multiuser multi-antenna cognitive network. Simulation results show that, by adopting the joint resource allocation schemes, obvious performance gain can be obtained over the traditional fixed strategies.", "subjects": "Information Theory (cs.IT)", "authors": "Xiaoming Chen, Chau Yuen,"}, 
{"urllink": "http://arxiv.org/abs/1406.0380", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0380", "title": "\nAn Algebraic Framework for the Real-Time Solution of Inverse Problems on  Embedded Systems", "abstract": "This article presents a new approach to the real-time solution of inverse problems on embedded systems. The class of problems addressed corresponds to ordinary differential equations (ODEs) with generalized linear constraints, whereby the data from an array of sensors forms the forcing function. The solution of the equation is formulated as a least squares (LS) problem with linear constraints. The LS approach makes the method suitable for the explicit solution of inverse problems where the forcing function is perturbed by noise. The algebraic computation is partitioned into a initial preparatory step, which precomputes the matrices required for the run-time computation; and the cyclic run-time computation, which is repeated with each acquisition of sensor data. The cyclic computation consists of a single matrix-vector multiplication, in this manner computation complexity is known a-priori, fulfilling the definition of a real-time computation. Numerical testing of the new method is presented on perturbed as well as unperturbed problems; the results are compared with known analytic solutions and solutions acquired from state-of-the-art implicit solvers. The solution is implemented with model based design and uses only fundamental linear algebra; consequently, this approach supports automatic code generation for deployment on embedded systems. The targeting concept was tested via software- and processor-in-the-loop verification on two systems with different processor architectures. Finally, the method was tested on a laboratory prototype with real measurement data for the monitoring of flexible structures. The problem solved is: the real-time overconstrained reconstruction of a curve from measured gradients. Such systems are commonly encountered in the monitoring of structures and/or ground subsidence.", "subjects": "Discrete Mathematics (cs.DM)", "authors": "Christoph Gugg, Matthew Harker, Paul O'Leary, Gerhard Rath,"}, 
{"urllink": "http://arxiv.org/abs/1405.2833", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2833", "title": "\nOn the Latency of Erasure-Coded Cloud Storage Systems", "abstract": "Distributed (Cloud) Storage Systems (DSS) exhibit heterogeneity in several dimensions such as the volume (size) of data, frequency of data access and the desired degree of reliability. Ultimately, the complex interplay between these dimensions impacts the latency performance of cloud storage systems. To this end, we propose and analyze a heterogeneous distributed storage model in which storage servers (disks) store the data of distinct classes. Data of class is encoded using a erasure code and the (random) data retrieval requests can also vary from class to class. We present a queuing theoretic analysis of the proposed model and establish upper and lower bounds on the average latency for each data class under various scheduling policies for data retrieval. Using simulations, we verify the accuracy of the proposed bounds and present qualitative insights which reveal the impact of heterogeneity and scheduling policies on the mean latency of different data classes. Lastly, we conclude with a discussion on per-class fairness in heterogeneous DSS.", "subjects": "Distributed, Parallel, and Cluster Computing (cs.DC)", "authors": "Akshay Kumar, Ravi Tandon, T. Charles Clancy,"}, 
{"urllink": "http://arxiv.org/abs/1406.7611", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7611", "title": "\nValidity of altmetrics data for measuring societal impact: A study using  data from Altmetric and F1000Prime", "abstract": "Can altmetric data be validly used for the measurement of societal impact? The current study seeks to answer this question with a comprehensive dataset (about 100,000 records) from very disparate sources (F1000, Altmetric, and an in-house database based on Web of Science). In the F1000 peer review system, experts attach particular tags to scientific papers which indicate whether a paper could be of interest for science or rather for other segments of society. The results show that papers with the tag \"good for teaching\" do achieve higher altmetric counts than papers without this tag - if the quality of the papers is controlled. At the same time, a higher citation count is shown especially by papers with a tag that is specifically scientifically oriented (\"new finding\"). The findings indicate that papers tailored for a readership outside the area of research should lead to societal impact. If altmetric data is to be used for the measurement of societal impact, the question arises of its normalization. In bibliometrics, citations are normalized for the papers' subject area and publication year. This study has taken a second analytic step involving a possible normalization of altmetric data. As the results show there are particular scientific topics which are of especial interest for a wide audience. Since these more or less interesting topics are not completely reflected in Thomson Reuters' journal sets, a normalization of altmetric data should not be based on the level of subject categories, but on the level of topics.", "subjects": "Digital Libraries (cs.DL)", "authors": "Lutz Bornmann,"}, 
{"urllink": "http://arxiv.org/abs/1406.0379", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0379", "title": "\nAn improved vulnerability index of complex networks based on fractal  dimension", "abstract": "With an increasing emphasis on network security, much more attention has been attracted to the vulnerability of complex networks. The multi-scale evaluation of vulnerability is widely used since it makes use of combined powers of the links' betweenness and has an effective evaluation to vulnerability. However, how to determine the coefficient in existing multi-scale evaluation model to measure the vulnerability of different networks is still an open issue. In this paper, an improved model based on the fractal dimension of complex networks is proposed to obtain a more reasonable evaluation of vulnerability with more physical significance. Not only the structure and basic physical properties of networks is characterized, but also the covering ability of networks, which is related to the vulnerability of the network, is taken into consideration in our proposed method. The numerical examples and real applications are used to illustrate the efficiency of our proposed method.", "subjects": "Social and Information Networks (cs.SI)", "authors": "Li Gou, Bo Wei, Rehan Sadiq, Sankaran Mahadevan, Yong Deng,"}, 
{"urllink": "http://arxiv.org/abs/1405.2826", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2826", "title": "\nFare Evasion in Transit Networks", "abstract": "Public transit systems in urban areas usually require large state subsidies, primarily due to high fare evasion rates. In this paper, we study new models for optimizing fare inspection strategies in transit networks based on bilevel programming. In the first level, the leader (the network operator) determines probabilities for inspecting passengers at different locations, while in the second level, the followers (the fare-evading passengers) respond by optimizing their routes given the inspection probabilities and travel times. To model the followers' behavior we study both a non-adaptive variant, in which passengers select a path a priori and continue along it throughout their journey, and an adaptive variant, in which they gain information along the way and use it to update their route. For these problems, which are interesting in their own right, we design exact and approximation algorithms and we prove a tight bound of 3/4 on the ratio of the optimal cost between adaptive and non-adaptive strategies. For the leader's optimization problem, we study a fixed-fare and a flexible-fare variant, where ticket prices may or may not be set at the operator's will. For the latter variant, we design an LP based approximation algorithm. Finally, using a local search procedure that shifts inspection probabilities within an initially determined support set, we perform an extensive computational study for all variants of the problem on instances of the Dutch railway and the Amsterdam subway network. This study reveals that our solutions are within 95% of theoretical upper bounds drawn from the LP relaxation.", "subjects": "Computer Science and Game Theory (cs.GT)", "authors": "Jos\u00e9 R. Correa, Tobias Harks, Vincent J.C. Kreuzen, Jannik Matuschke,"}, 
{"urllink": "http://arxiv.org/abs/1406.7608", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7608", "title": "\nParameterized Synthesis Case Study: AMBA AHB (extended version)", "abstract": "We revisit the AMBA AHB case study that has been used as a benchmark for several reactive syn- thesis tools. Synthesizing AMBA AHB implementations that can serve a large number of masters is still a difficult problem. We demonstrate how to use parameterized synthesis in token rings to obtain an implementation for a component that serves a single master, and can be arranged in a ring of arbitrarily many components. We describe new tricks -- property decompositional synthesis, and direct encoding of simple GR(1) -- that together with previously described optimizations allowed us to synthesize the model with 14 states in 30 minutes.", "subjects": "Software Engineering (cs.SE)", "authors": "Roderick Bloem, Swen Jacobs, Ayrat Khalimov,"}, 
{"urllink": "http://arxiv.org/abs/1406.0375", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.0375", "title": "\nAssessment Model for Opportunistic Routing", "abstract": "This paper proposes an assessment model, based on a new taxonomy, which comprises an evaluation guideline with performance metrics and experimental setup to aid designers in evaluating solutions through fair comparisons. Simulation results are provided based on the proposed model considering Epidemic, PROPHET, Bubble Rap, and Spray and Wait, and showing how they perform under the same set of metrics and scenario", "subjects": "Networking and Internet Architecture (cs.NI)", "authors": "Waldir Moreira, Paulo Mendes, Susana Sargento,"}, 
{"urllink": "http://arxiv.org/abs/1405.2822", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2822", "title": "\nImitation-based Social Spectrum Sharing", "abstract": "Dynamic spectrum sharing is a promising technology for improving the spectrum utilization. In this paper, we study how secondary users can share the spectrum in a distributed fashion based on social imitations. The imitation-based mechanism leverages the social intelligence of the secondary user crowd and only requires a low computational power for each individual user. We introduce the information sharing graph to model the social information sharing relationship among the secondary users. We propose an imitative spectrum access mechanism on a general information sharing graph such that each secondary user first estimates its expected throughput based on local observations, and then imitates the channel selection of another neighboring user who achieves a higher throughput. We show that the imitative spectrum access mechanism converges to an imitation equilibrium, where no beneficial imitation can be further carried out on the time average. Numerical results show that the imitative spectrum access mechanism can achieve efficient spectrum utilization and meanwhile provide good fairness across secondary users.", "subjects": "Networking and Internet Architecture (cs.NI)", "authors": "Xu Chen, Jianwei Huang,"}, 
{"urllink": "http://arxiv.org/abs/1406.7589", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7589", "title": "\nLearning from Others, Together: Brokerage, Closure and Team Performance", "abstract": "Scholarship on teams has focused on the relationship between a team's performance, however defined, and the network structure among team members. For example, Uzzi and Spiro (2005) find that the creative performance of Broadway musical teams depends heavily on the internal cohesion of team members and their past collaborative experience with individuals outside their immediate teams. In other words, team members' internal cohesion and external ties are crucial to the team's success. How, then, do they interact to produce positive performance outcomes? In our work, we separate the proximal causes of tie formation from the proximal determinants of outcomes to determine the mechanism behind this interaction. To examine this puzzle, we examine the performance of national soccer squads over time as a function of changing levels and configurations of brokerage and closure ties formed by players working for professional soccer clubs.", "subjects": "Social and Information Networks (cs.SI)", "authors": "Jose Uribe, Dan Wang,"}, 
{"urllink": "http://arxiv.org/abs/1405.2820", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1405.2820", "title": "\nA weight-distribution bound for entropy extractors using linear binary  codes", "abstract": "We consider a bound on the bias reduction of a random number generator by processing based on binary linear codes. We introduce a new bound on the total variation distance of the processed output based on the weight distribution of the code generated by the chosen binary matrix. Starting from this result we show a lower bound for the entropy rate of the output of linear binary extractors.", "subjects": "Information Theory (cs.IT)", "authors": "Alessio Meneghetti, Massimiliano Sala, Alessandro Tomasi,"}, 
{"urllink": "http://arxiv.org/abs/1406.7588", "category": "Computer Science ", "pdflink": "http://arxiv.org/pdf/1406.7588", "title": "\nLessons Learned from an Experiment in Crowdsourcing Complex Citizen  Engineering Tasks with Amazon Mechanical Turk", "abstract": "We investigate the feasibility of obtaining highly trustworthy results using crowdsourcing on complex engineering tasks. Crowdsourcing is increasingly seen as a potentially powerful way of increasing the supply of labor for solving society's problems. While applications in domains such as citizen-science, citizen-journalism or knowledge organization (e.g., Wikipedia) have seen many successful applications, there have been fewer applications focused on solving engineering problems, especially those involving complex tasks. This may be in part because of concerns that low quality input into engineering analysis and design could result in failed structures leading to loss of life. We compared the quality of work of the anonymous workers of Amazon Mechanical Turk (AMT), an online crowdsourcing service, with the quality of work of expert engineers in solving the complex engine